# Дообучение мультимодальной модели LLaVA-Gemma на русском языке

Данный проект реализует дообучение мультимодальной модели LLaVA-Gemma-2B на русскоязычных данных в рамках практики VK x RUDN. Проект использует открытые данные и модели от VK, включает полный пайплайн от загрузки данных до оценки качества дообученной модели.

## Структура проекта

- `main.ipynb` — основной ноутбук с реализацией пайплайна:
  - Загрузка и тестирование базовой модели
  - Подготовка данных для дообучения
  - Дообучение с использованием LoRA
  - Оценка дообученной модели
  - Сравнительный анализ результатов
- `project_description.md` — подробное техническое описание методологии и результатов
- `requirements.txt` — список зависимостей проекта
- `llava-gemma-finetuned_w_torchvision/` — папка с сохранёнными весами дообученной модели (не включена в репозиторий)
- `coco/` — локальное хранилище изображений COCO 2017 (не включено в репозиторий)
  - `train2017/` — тренировочные изображения
  - `val2014/` — валидационные изображения

## Цель проекта

Дообучить мультимодальную модель LLaVA-Gemma-2B на русскоязычном датасете инструкций с изображениями и провести оценку качества на специализированных бенчмарках для визуально-языкового понимания.

## Задачи проекта

1. Изучить и загрузить базовую модель `deepvk/llava-gemma-2b-lora`
2. Подготовить данные из датасета `deepvk/LLaVA-Instruct-ru` для дообучения
3. Реализовать дообучение модели с использованием метода LoRA (Low-Rank Adaptation)
4. Провести оценку дообученной модели и сравнить результаты с baseline
5. Подготовить документацию с описанием методологии и результатов

## Используемые данные и ресурсы

### Модель

- **Базовая модель**: `deepvk/llava-gemma-2b-lora` (Hugging Face)
- **Архитектура**: LLaVA на базе Gemma-2B
- **Vision Encoder**: CLIP ViT (заморожен при дообучении)
- **Language Model**: Gemma-2B с LoRA-адаптацией

### Датасеты (открытые данные VK)

Все используемые датасеты доступны на Hugging Face от VK:

- **`deepvk/LLaVA-Instruct-ru`** — датасет для дообучения
  - Формат: диалоги (conversations) с привязкой к изображениям COCO
  - Размер: 109,905 тренировочных примеров, 34,075 валидационных примеров
  - Использование: дообучение модели на русскоязычных инструкциях

- **`deepvk/GQA-ru`** — бенчмарк для оценки
  - Тип: визуальные вопросы с краткими ответами
  - Метрика: ExactMatch с лемматизацией (pymorphy2)
  - Размер: 12,216 вопросов (testdev)
  - Использование: оценка качества ответов на вопросы по изображениям

- **`deepvk/MMBench-ru`** — бенчмарк для оценки
  - Тип: multiple choice задачи по изображениям
  - Метрика: Accuracy
  - Размер: 3,910 примеров (dev)
  - Использование: оценка понимания изображений в формате выбора варианта ответа

### Локальные данные

- **COCO 2017**: изображения размещены локально в папке `coco/`
  - `train2017/` — тренировочные изображения COCO
  - `val2014/` — валидационные изображения COCO

## Методология

### Метод дообучения: LoRA (Low-Rank Adaptation)

LoRA позволяет эффективно дообучать большие модели, добавляя низкоранговые адаптеры к целевым слоям, при этом основные веса остаются замороженными.

**Конфигурация LoRA:**
- `r` (rank) = 64
- `lora_alpha` = 128
- `lora_dropout` = 0.05
- Target modules: `q_proj`, `k_proj`, `v_proj`, `o_proj`, `gate_proj`, `up_proj`, `down_proj`
- Обучаемые параметры: ~88M (3.0% от всех параметров модели)

### Гиперпараметры обучения

- Эпохи: 1
- Batch size: 1 (per device)
- Gradient accumulation steps: 8 (эффективный batch size = 8)
- Learning rate: 1e-4
- LR scheduler: cosine с warmup (warmup_ratio=0.03)
- Precision: FP16
- Gradient checkpointing: включён для экономии памяти
- Evaluation: каждые 50 шагов
- Сохранение: каждые 50 шагов, сохранение последних 10 чекпоинтов
- Размер выборки: 2000 тренировочных примеров, 25 валидационных примеров

### Оценка на бенчмарках

**GQA-ru:**
- Метрика: ExactMatch по лемматизированным формам (используется pymorphy2)
- Промпт включает инструкцию "Ответь одним словом"
- Генерация: max_new_tokens=10

**MMBench-ru:**
- Метрика: Accuracy для выбора одной буквы (A/B/C/D)
- Генерация: max_new_tokens=5
- Извлечение: первая встреченная буква из ABCD

## Результаты

### Baseline метрики

| Бенчмарк | Метрика | Значение |
|----------|---------|----------|
| GQA-ru | ExactMatch | 0.4637 (46.37%) |
| MMBench-ru | Accuracy | 0.4019 (40.19%) |

### Результаты после дообучения

| Бенчмарк | Метрика | Значение | Изменение | Изменение (%) |
|----------|---------|----------|-----------|---------------|
| GQA-ru | ExactMatch | 0.4637 (46.37%) | +0.0083 | +1.8% |
| MMBench-ru | Accuracy | 0.4019 (40.19%) | +0.2101 | +52.3% |

### Анализ результатов

Полученные результаты демонстрируют неоднозначный эффект дообучения, что ожидаемо при использовании ограниченной выборки (2000 тренировочных примеров) и одной эпохи обучения.

На бенчмарке GQA-ru качество модели практически не изменилось относительно baseline. Абсолютное улучшение метрики ExactMatch составляет порядка +0.8 процентных пункта, что находится в пределах статистического шума для данного типа задач и размера выборки.
Это указывает на то, что:
- модель уже обладает хорошими базовыми знаниями для кратких визуальных вопросов,
- текущий объём дообучения недостаточен для заметного улучшения обобщающей способности на GQA-ru,
- формат инструкций LLaVA-Instruct-ru лишь частично соответствует формату вопросов GQA.

На бенчмарке MMBench-ru наблюдается существенное улучшение качества: рост accuracy более чем на 20 процентных пунктов, что соответствует относительному увеличению свыше 50% по сравнению с baseline.
Данный эффект можно объяснить следующими факторами:
- формат MMBench-ru (multiple choice с визуальным контекстом) хорошо согласуется с инструкционным стилем датасета LLaVA-Instruct-ru;
- дообучение улучшает способность модели следовать инструкциям и выбирать релевантный вариант ответа;
- LoRA-адаптация эффективно корректирует языковую часть модели без разрушения визуальных представлений.

Общий вывод
В целом результаты подтверждают корректность выбранного подхода:
- дообучение не приводит к деградации качества на одном из ключевых бенчмарков (GQA-ru);
- при этом достигается значительный прирост на задаче инструкционного визуального понимания (MMBench-ru).

## Требования

- Python 3.10+
- PyTorch с поддержкой CUDA (рекомендуется CUDA 11.8+)
- GPU с минимум 16 GB VRAM
- Около 50 GB дискового пространства (модель + COCO изображения)

## Установка и запуск

1. **Установка зависимостей:**
   ```bash
   pip install -r requirements.txt
   ```

2. **Подготовка данных:**
   - Скачайте изображения COCO 2017
   - Поместите их в папку `coco/` со следующей структурой:
     ```
     coco/
     ├── train2017/  # тренировочные изображения
     └── val2014/    # валидационные изображения
     ```

3. **Запуск ноутбука:**
   - Откройте `main.ipynb`
   - Выполняйте ячейки последовательно
   - Результаты обучения сохраняются в `llava-gemma-finetuned_w_torchvision/`

## Особенности реализации

- Использование `torchvision.read_image` для эффективной загрузки изображений
- Кастомный `LLaVADataCollator` для обработки мультимодальных данных
- Оптимизация памяти CUDA через gradient checkpointing и FP16
- Автоматическая заморозка vision encoder при дообучении
- Реализация функций оценки с поддержкой лемматизации для русского языка

## Воспроизводимость

Все эксперименты используют фиксированный seed (42) для обеспечения воспроизводимости результатов:
- `transformers.set_seed(42)`
- `random.seed(42)`
- `numpy.random.seed(42)`
- `torch.manual_seed(42)`

## Дополнительные материалы

Подробное описание методологии, архитектуры решения и деталей реализации можно найти в файле `project_description.md`.
