# Дообучение мультимодальной модели LLaVA-Gemma на русском языке

## 1. Введение

Данный проект выполнен в рамках практики VK x RUDN и посвящён дообучению мультимодальной модели `deepvk/llava-gemma-2b-lora` на русскоязычном датасете с целью повышения качества в задачах визуально-языкового понимания. Работа включает полный цикл: от изучения базовой модели и оценки её качества до подготовки данных, дообучения с использованием LoRA и сравнительной оценки результатов.

Проект использует открытые данные от VK, доступные на Hugging Face:
- Датасет для дообучения: `deepvk/LLaVA-Instruct-ru`
- Бенчмарки для оценки: `deepvk/GQA-ru` и `deepvk/MMBench-ru`

---

## 2. Цель и задачи проекта

### 2.1 Цель проекта

Дообучить мультимодальную модель LLaVA-Gemma-2B на русскоязычном датасете инструкций с изображениями и провести тщательную оценку качества дообученной модели на специализированных бенчмарках для визуально-языкового понимания.

### 2.2 Задачи проекта

1. **Изучение базовой модели**: Настроить и загрузить базовую модель `deepvk/llava-gemma-2b-lora` для инференса и дообучения, изучить её архитектуру и возможности.

2. **Подготовка данных**: Загрузить и подготовить данные из датасета `deepvk/LLaVA-Instruct-ru`, включая обработку изображений COCO из локальной структуры `coco/train2017/` и `coco/val2014/`.

3. **Дообучение модели**: Реализовать дообучение с использованием метода LoRA (Low-Rank Adaptation), настроить процесс обучения с оптимальными гиперпараметрами.

4. **Оценка дообученной модели**: Провести оценку дообученной модели на тех же бенчмарках и выполнить сравнительный анализ результатов.

5. **Документирование**: Подготовить подробное описание методологии, результатов и рекомендаций по дальнейшему улучшению.

### 2.3 Ожидаемые результаты

- Воспроизводимый пайплайн дообучения мультимодальной модели
- Обученная модель, сохранённая в виде чекпоинтов
- Отчёт по метрикам baseline и дообученной модели
- Рекомендации по дальнейшему улучшению качества

---

## 3. Используемые данные и ресурсы

### 3.1 Модель

| Параметр | Значение |
|----------|----------|
| Название | `deepvk/llava-gemma-2b-lora` |
| Архитектура | LLaVA на базе Gemma-2B |
| Vision Encoder | CLIP ViT (заморожен при дообучении) |
| Language Model | Gemma-2B с LoRA-адаптацией |
| Количество параметров | ~2.9 млрд (всего), ~88M обучаемых (3.0%) |

### 3.2 Датасеты (открытые данные VK)

Все датасеты доступны на Hugging Face и предоставлены VK:

#### Датасет для дообучения

| Параметр | Значение |
|----------|----------|
| Название | `deepvk/LLaVA-Instruct-ru` |
| Формат | Диалоги (conversations) с привязкой к изображениям COCO |
| Размер train | 109,905 примеров |
| Размер validation | 34,075 примеров |
| Использование | Дообучение модели на русскоязычных инструкциях |

#### Бенчмарки для оценки

| Бенчмарк | Описание | Метрика | Размер |
|----------|----------|---------|--------|
| `deepvk/GQA-ru` | Визуальные вопросы с краткими ответами | ExactMatch (с лемматизацией) | 12,216 вопросов (testdev) |
| `deepvk/MMBench-ru` | Multiple choice задачи по изображениям | Accuracy | 3,910 примеров (dev) |

### 3.3 Локальные данные

- **COCO 2017**: изображения размещены локально в папке `coco/`
  - `train2017/` — 118,287 тренировочных изображений
  - `val2014/` — 40,458 валидационных изображений

---

## 4. Методология

### 4.1 Метод дообучения: LoRA (Low-Rank Adaptation)

LoRA (Low-Rank Adaptation) — эффективный метод параметрически-эффективного дообучения больших языковых моделей. Метод добавляет низкоранговые матрицы-адаптеры к целевым слоям модели, которые обучаются, в то время как исходные веса остаются замороженными.

**Преимущества LoRA:**
- Эффективное использование памяти: обучается только небольшая доля параметров
- Быстрое обучение: меньше вычислений по сравнению с полным fine-tuning
- Возможность использования нескольких адаптеров для разных задач

**Конфигурация LoRA (используется в проекте):**

| Параметр | Значение | Описание |
|----------|----------|----------|
| `r` (rank) | 64 | Ранг матриц адаптации |
| `lora_alpha` | 128 | Коэффициент масштабирования |
| `lora_dropout` | 0.05 | Dropout для регуляризации |
| Target modules | `q_proj`, `k_proj`, `v_proj`, `o_proj`, `gate_proj`, `up_proj`, `down_proj` | Целевые слои для адаптации |

**Статистика параметров:**
- Обучаемые параметры: ~88M (3.0% от общего числа)
- Замороженные параметры: ~2.8 млрд (97.0%)

### 4.2 Архитектура решения

```
LLaVA-Gemma-2B (дообучение)
├── Vision Encoder (CLIP ViT) - заморожен
├── Multi-modal Projector - заморожен  
└── Language Model (Gemma-2B) - дообучается через LoRA
    ├── Attention: q_proj, k_proj, v_proj, o_proj (LoRA адаптеры)
    └── MLP: gate_proj, up_proj, down_proj (LoRA адаптеры)
```

**Особенности реализации:**
- Vision encoder полностью заморожен (все параметры имеют `requires_grad=False`)
- Vision encoder работает в режиме `eval()` для экономии памяти
- Multi-modal projector также заморожен
- LoRA адаптеры применяются только к языковой модели

### 4.3 Оценка на бенчмарках

#### 4.3.1 Оценка на GQA-ru

**Метрика:** ExactMatch — точное совпадение предсказанного ответа с эталонным после лемматизации.

**Особенности реализации:**
- Для обработки морфологии русского языка используется лемматизация с помощью `pymorphy2`
- Промпт содержит инструкцию "Ответь одним словом" для получения кратких ответов
- Ответ очищается от пунктуации и приводится к нижнему регистру
- Генерация: `max_new_tokens=10`
- Инференс выполняется в режиме `torch.no_grad()`

**Алгоритм оценки:**
1. Загрузка изображений и вопросов из датасета
2. Формирование промпта с вопросом и инструкцией
3. Генерация ответа моделью
4. Извлечение и очистка предсказанного ответа
5. Лемматизация предсказания и эталонного ответа
6. Сравнение лемматизированных форм

#### 4.3.2 Оценка на MMBench-ru

**Метрика:** Accuracy — доля правильно выбранных вариантов ответа.

**Формат задачи:** Multiple choice с вариантами A, B, C, D.

**Алгоритм оценки:**
1. Формирование промпта с вопросом, подсказкой (hint) и вариантами ответа
2. Генерация ответа моделью (`max_new_tokens=5`)
3. Извлечение буквы ответа (A, B, C или D) из сгенерированного текста
4. Сравнение с правильным вариантом

---

## 5. Структура кода

Реализация представлена в Jupyter Notebook `main.ipynb` и включает следующие разделы:

1. **Импорты и фиксация seed** — загрузка необходимых библиотек, настройка воспроизводимости
2. **Загрузка baseline модели** — инициализация LLaVA-Gemma с настройкой процессора
3. **Загрузка бенчмарков** — загрузка GQA-ru и MMBench-ru датасетов
4. **Функции оценки** — реализация `evaluate_on_gqa()` и `evaluate_on_mmbench()`
5. **Загрузка датасета для дообучения** — загрузка LLaVA-Instruct-ru
6. **Подготовка данных** — утилиты для загрузки изображений COCO, форматирование диалогов
7. **Дообучение модели** — конфигурация LoRA, настройка Trainer, запуск обучения
8. **Оценка дообученной модели** — оценка на бенчмарках после дообучения
9. **Сравнение результатов** — таблица и графики сравнения baseline vs дообученной модели

### 5.1 Ключевые компоненты реализации

**Data Collator (`LLaVADataCollator`):**
- Использует `torchvision.read_image` для эффективной загрузки изображений
- Обрабатывает батчи изображений и текстов
- Формирует labels с маскировкой padding токенов

**Функции форматирования:**
- `get_coco_image_path()` — поиск пути к изображению в локальной структуре COCO
- `format_conversation()` — преобразование диалога из датасета в формат для обучения LLaVA

**Оптимизация памяти:**
- Использование FP16 precision
- Gradient checkpointing
- Заморозка vision encoder
- Очистка CUDA cache между операциями

---

## 6. Гиперпараметры обучения

| Параметр | Значение |
|----------|----------|
| Эпохи | 1 |
| Batch size (per device) | 1 |
| Gradient accumulation steps | 8 |
| Эффективный batch size | 8 |
| Learning rate | 1e-4 |
| LR scheduler | cosine |
| Warmup ratio | 0.03 |
| Weight decay | 0.0 |
| Precision | FP16 |
| Gradient checkpointing | True |
| Optimizer | adamw_torch |
| Evaluation strategy | steps (каждые 50 шагов) |
| Save strategy | steps (каждые 50 шагов) |
| Save total limit | 10 |
| Размер тренировочной выборки | 2000 примеров |
| Размер валидационной выборки | 25 примеров |
| Max sequence length | 384 токена |

**Примечания:**
- Использован небольшой размер выборки для демонстрации подхода
- Для полного обучения рекомендуется использовать больше данных
- Gradient checkpointing и FP16 необходимы для работы на GPU с 16GB VRAM

---

## 7. Результаты

### 7.1 Baseline метрики

| Бенчмарк | Метрика | Значение |
|----------|---------|----------|
| GQA-ru | ExactMatch | 0.4637 (46.37%) |
| MMBench-ru | Accuracy | 0.4019 (40.19%) |

### 7.2 Результаты после дообучения

| Бенчмарк | Метрика | Значение | Изменение | Изменение (%) |
|----------|---------|----------|-----------|---------------|
| GQA-ru | ExactMatch | 0.4637 (46.37%) | +0.0083 | +1.8% |
| MMBench-ru | Accuracy | 0.4019 (40.19%) | +0.2101 | +52.3% |

### 7.3 Анализ результатов

Полученные результаты демонстрируют различный эффект дообучения на рассматриваемых бенчмарках, что согласуется с характером используемых данных и ограничениями эксперимента.

GQA-ru.
На бенчмарке GQA-ru значение метрики ExactMatch после дообучения изменилось незначительно и осталось на уровне baseline. Наблюдаемое небольшое улучшение находится в пределах статистической погрешности и указывает на то, что дообучение на ограниченной подвыборке (10000 примеров) и в течение одной эпохи не оказывает существенного влияния на способность модели отвечать на краткие фактологические визуальные вопросы.

MMBench-ru.
В то же время на бенчмарке MMBench-ru зафиксировано существенное улучшение accuracy. Рост метрики более чем на 20 процентных пунктов свидетельствует о том, что дообучение эффективно улучшает способность модели следовать инструкциям и выбирать корректный вариант ответа в задачах multiple-choice с визуальным контекстом.

Общее наблюдение.
Анализ кривых обучения показывает, что значение training loss и eval loss стабилизируется к концу эпохи, что указывает на выход процесса обучения на плато. Это означает, что при текущей конфигурации гиперпараметров и объёме данных модель исчерпала потенциал улучшения качества в рамках одного прохода по выборке.

---

## 8. Воспроизводимость

### 8.1 Фиксация seed

Для обеспечения воспроизводимости результатов фиксируется seed во всех источниках случайности:

- `transformers.set_seed(42)`
- `random.seed(42)`
- `numpy.random.seed(42)`
- `torch.manual_seed(42)`
- `torch.cuda.manual_seed_all(42)`
- `torch.backends.cudnn.deterministic = True`
- `torch.backends.cudnn.benchmark = False`
- `TrainingArguments(seed=42)`

### 8.2 Требования

- Python 3.10+
- PyTorch с поддержкой CUDA (рекомендуется CUDA 11.8+)
- GPU с минимум 16 GB VRAM
- Около 50 GB дискового пространства (модель + COCO изображения)
- Зависимости из `requirements.txt`

### 8.3 Установка и запуск

1. Установите зависимости:
   ```bash
   pip install -r requirements.txt
   ```

2. Подготовьте данные:
   - Скачайте изображения COCO 2017
   - Поместите их в папку `coco/` с поддиректориями `train2017/` и `val2014/`

3. Запустите ноутбук:
   - Откройте `main.ipynb`
   - Выполняйте ячейки последовательно
   - Результаты обучения сохраняются в `llava-gemma-finetuned_w_torchvision/`

---

## 9. Выводы

В ходе работы была реализована и протестирована методология дообучения мультимодальной модели LLaVA-Gemma-2B на русскоязычном датасете инструкций с изображениями. В рамках проекта были выполнены следующие этапы:

1. Загружена и настроена базовая модель deepvk/llava-gemma-2b-lora для инференса и дообучения.
2. Реализованы функции оценки качества на бенчмарках GQA-ru и MMBench-ru с учётом морфологических особенностей русского языка.
3. Настроено параметрически-эффективное дообучение с использованием LoRA, что позволило обучать модель на GPU с ограниченным объёмом памяти.
4. Проведено дообучение модели на подвыборке из 2000 примеров датасета LLaVA-Instruct-ru.
5. Выполнена сравнительная оценка baseline и дообученной модели.

Эксперимент показал, что:
дообучение не приводит к деградации качества на бенчмарке GQA-ru;
на бенчмарке MMBench-ru достигается существенное улучшение качества, что указывает на успешную адаптацию модели к задачам инструкционного визуально-языкового понимания;
динамика training loss и eval loss свидетельствует о выходе процесса обучения на плато к концу эпохи.

Полученные результаты подтверждают корректность реализованного пайплайна и выбранного метода дообучения. Для дальнейшего повышения качества модели целесообразно:
- увеличить объём обучающей выборки,
- рассмотреть обучение в несколько эпох,
- провести подбор гиперпараметров LoRA и learning rate,
- расширить набор бенчмарков для оценки обобщающей способности модели.